{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/html/body/div/div[2]/div[2]/div[2]/div[1]/div[1]/ul/li[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<li class=\"sa_item _SECTION_HEADLINE\">\n",
    "\t\t\t<div class=\"sa_item_inner\">\n",
    "\t\t\t\t<div class=\"sa_item_flex _LAZY_LOADING_WRAP\">\n",
    "\t\t\t\t\t<div class=\"sa_thumb _LAZY_LOADING_ERROR_HIDE\">\n",
    "\t\t\t\t\t\t<div class=\"sa_thumb_inner\">\n",
    "\t\t\t\t\t\t\t<a href=\"https://n.news.naver.com/mnews/article/654/0000097030\" class=\"sa_thumb_link _NLOG_IMPRESSION\" data-clk=\"pol.clart\" data-imp-gdid=\"88221sgc_000000000000000000097030\" data-imp-url=\"https://n.news.naver.com/mnews/article/654/0000097030\" data-imp-index=\"1\">\n",
    "\t\t\t\t\t\t\t\t<img class=\"_LAZY_LOADING _LAZY_LOADING_INIT_HIDE\" width=\"110\" height=\"75\" alt=\"[속보] 민주 &quot;尹 광기 더는 용납 못해…與 탄핵 거부는 국민에 대한 반역&quot;\" style=\"\" src=\"https://mimgnews.pstatic.net/image/origin/654/2024/12/14/97030.jpg?type=nf220_150\">\n",
    "\t\t\t\t\t\t\t</a>\n",
    "\t\t\t\t\t\t</div>\n",
    "\t\t\t\t\t</div>\n",
    "\t\t\t\t\t<div class=\"sa_text\">\n",
    "\t\t\t\t\t\t<a href=\"https://n.news.naver.com/mnews/article/654/0000097030\" class=\"sa_text_title _NLOG_IMPRESSION\" data-clk=\"pol.clart\" data-imp-gdid=\"88221sgc_000000000000000000097030\" data-imp-url=\"https://n.news.naver.com/mnews/article/654/0000097030\" data-imp-index=\"1\">\n",
    "\t\t\t\t\t\t\t<strong class=\"sa_text_strong\">[속보] 민주 \"尹 광기 더는 용납 못해…與 탄핵 거부는 국민에 대한 반역\"</strong>\n",
    "\t\t\t\t\t\t</a>\n",
    "\t\t\t\t\t\t<div class=\"sa_text_lede\">더불어민주당은 14일 윤석열 대통령에 대한 두 번째 탄핵소추안이 표결을 앞두고 \"탄핵 거부는 국민에 대한 반역\"이라며 국민의힘을 향해 동참을 촉구했다. 민주당 황정아 대변인은 브리핑에서 \"헌법과 법치, 민주주의와 대</div>\n",
    "\t\t\t\t\t\t<div class=\"sa_text_info\">\n",
    "\t\t\t\t\t\t\t<div class=\"sa_text_info_left\">\n",
    "\t\t\t\t\t\t\t\t<div class=\"sa_text_press\">강원도민일보</div>\n",
    "\t\t\t\t\t\t\t\t<a href=\"https://n.news.naver.com/mnews/article/comment/654/0000097030\" class=\"sa_text_cmt _COMMENT_COUNT_LIST\" style=\"\" data-ticket=\"news\" data-object-id=\"news654,0000097030\" data-zero-allow=\"false\" data-processed=\"true\">100<span class=\"sa_text_symbol\"><span class=\"blind\">이상</span>+</span></a>\n",
    "\t\t\t\t\t\t\t</div>\n",
    "\t\t\t\t\t\t\t<div class=\"sa_text_info_right\">\n",
    "\t\t\t\t\t\t\t\t<a href=\"/cluster/c_202412140950_00000001/section/100?oid=654&amp;aid=0000097030\" class=\"sa_text_cluster\" data-clk=\"clcou\">\n",
    "\t\t\t\t\t\t\t\t\t<span class=\"sa_text_cluster_num\">22</span>\n",
    "\t\t\t\t\t\t\t\t\t<span class=\"blind\">개의 관련뉴스 더보기</span>\n",
    "\t\t\t\t\t\t\t\t</a>\n",
    "\t\t\t\t\t\t\t</div>\n",
    "\t\t\t\t\t\t</div>\n",
    "\t\t\t\t\t</div>\n",
    "\t\t\t\t</div>\n",
    "\t\t\t</div>\n",
    "\t\t</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def set_chromedriver(path: str = \"/usr/bin/chromedriver\"):\n",
    "    global driver\n",
    "    # 크롬 드라이버 사용\n",
    "    service = Service(executable_path=path)  # selenium 최근 버전은 이렇게 해야함.\n",
    "\n",
    "    # 이런 것 때문에 기술문서를 보면서 코딩해야하고 영어도 잘해야함. (by. 스터디 팀장님)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=options, service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date):\n",
    "    ymd, dn, hm = date.split(\" \")\n",
    "    ymd = ymd[:-1].replace(\".\", \"-\")\n",
    "    hours = 12 if dn == \"오후\" else 0\n",
    "    h = int(hm.split(\":\")[0]) + hours\n",
    "    m = hm.split(\":\")[1]\n",
    "    return f\"{ymd} {h}:{m}\"\n",
    "\n",
    "\n",
    "# def validate_news_url(url: str) -> bool:\n",
    "#     \"\"\"\n",
    "#     Validate if the URL matches expected Naver news patterns.\n",
    "#     \"\"\"\n",
    "#     valid_patterns = [\n",
    "#         # Regular news articles\n",
    "#         r\"https?://n\\.news\\.naver\\.com/article/\\d+\",\n",
    "#         r\"https?://news\\.naver\\.com/main/read\\.naver\\?mode=LSD&mid=shm&sid1=\\d+&oid=\\d+&aid=\\d+\",\n",
    "#         # Sports news\n",
    "#         r\"https?://sports\\.news\\.naver\\.com/.+/news/read\\.nhn\\?oid=\\d+&aid=\\d+\",\n",
    "#         # Entertainment news\n",
    "#         r\"https?://entertain\\.naver\\.com/read\\?oid=\\d+&aid=\\d+\",\n",
    "#     ]\n",
    "\n",
    "#     return any(re.match(pattern, url) for pattern in valid_patterns)\n",
    "\n",
    "\n",
    "# def extract_news_info(url: str) -> dict:\n",
    "#     \"\"\"\n",
    "#     Extract news article identifiers from URL for validation.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Extract oid (언론사 ID) and aid (기사 ID)\n",
    "#         oid_match = re.search(r\"oid=(\\d+)\", url)\n",
    "#         aid_match = re.search(r\"aid=(\\d+)\", url)\n",
    "\n",
    "#         if oid_match and aid_match:\n",
    "#             return {\"oid\": oid_match.group(1), \"aid\": aid_match.group(1)}\n",
    "#         return None\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# def validate_news_metadata(info: dict) -> bool:\n",
    "#     \"\"\"\n",
    "#     Validate crawled news metadata.\n",
    "#     \"\"\"\n",
    "#     required_fields = [\"title\", \"content\", \"press\", \"date\"]\n",
    "#     if not all(field in info for field in required_fields):\n",
    "#         return False\n",
    "\n",
    "#     # Check if content is not too short (possible crawl error)\n",
    "#     if len(info[\"content\"]) < 100:  # Minimum content length\n",
    "#         return False\n",
    "\n",
    "#     return True\n",
    "\n",
    "\n",
    "def _get_class_info(\n",
    "    driver,\n",
    "    class_name,\n",
    "):\n",
    "    return driver.find_element(By.CLASS_NAME, class_name).text\n",
    "\n",
    "\n",
    "# def get_news_info(\n",
    "#     url: str,\n",
    "#     category: str,\n",
    "# ):\n",
    "#     driver.get(url)\n",
    "\n",
    "#     driver.implicitly_wait(\n",
    "#         2\n",
    "#     )  # 2초 안에 웹페이지를 load하면 바로 넘어가거나, 2초를 기다림\n",
    "#     # 사전 준비\n",
    "#     email_pattern = (\n",
    "#         r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}\"  # 이메일 정규표현식\n",
    "#     )\n",
    "#     reporter_pattern = r\"[ㄱ-힣]{2,4}\"  # 기자 이름 정규표현식\n",
    "\n",
    "#     if category in [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\"]:\n",
    "#         class_dict = {\n",
    "#             \"title\": \"media_end_head_head_title\",\n",
    "#             \"content\": \"_article_content\",\n",
    "#             \"email_reporter\": \"byline\",\n",
    "#         }\n",
    "#         # title = _get_class_info(driver, class_dict[\"title\"])\n",
    "#         # content = _get_class_info(driver, class_dict[\"content\"])\n",
    "#         title = driver.find_element(By.CLASS_NAME, \"media_end_head_title\").text\n",
    "#         content = driver.find_element(By.CLASS_NAME, \"_article_content\").text\n",
    "#         email_text = driver.find_element(\n",
    "#             By.CLASS_NAME, class_dict[\"email_reporter\"]\n",
    "#         ).text\n",
    "#         email = re.findall(email_pattern, email_text)[0]\n",
    "#         reporter = re.findall(reporter_pattern, email_text)[0]\n",
    "\n",
    "#         press = driver.find_element(\n",
    "#             By.CLASS_NAME, \"media_journalistcard_summary_press_img\"\n",
    "#         ).get_attribute(\"alt\")\n",
    "\n",
    "#         date_results = driver.find_elements(\n",
    "#             By.CLASS_NAME, \"media_end_head_info_datestamp_bunch\"\n",
    "#         )\n",
    "\n",
    "#         if len(date_results) > 1:\n",
    "#             date = driver.find_element(\n",
    "#                 By.CLASS_NAME, \"_ARTICLE_MODIFY_DATE_TIME\"\n",
    "#             ).get_attribute(\"data-modify-date-time\")\n",
    "#         else:\n",
    "#             date = driver.find_element(\n",
    "#                 By.CLASS_NAME, \"_ARTICLE_DATE_TIME\"\n",
    "#             ).get_attribute(\"data-date-time\")\n",
    "\n",
    "#     elif category in [\"스포츠\", \"연예\"]:\n",
    "#         class_dict = dict(\n",
    "#             title=\"NewsEndMain_article_head_title__ztaL4\",\n",
    "#             content=\"_article_content\",\n",
    "#             email_reporter=\"NewsEndMain_article_journalist_info__Cdr3D\",\n",
    "#         )\n",
    "#         title, content = _get_class_info(driver, class_dict[\"title\"]), _get_class_info(\n",
    "#             driver, class_dict[\"content\"]\n",
    "#         )\n",
    "\n",
    "#         email_text = driver.find_element(\n",
    "#             By.CLASS_NAME, class_dict[\"email_reporter\"]\n",
    "#         ).text\n",
    "#         email = re.findall(email_pattern, email_text)[0]\n",
    "#         reporter = re.findall(reporter_pattern, email_text)[0]\n",
    "\n",
    "#         if category == \"스포츠\":\n",
    "#             press = (\n",
    "#                 driver.find_element(\n",
    "#                     By.CLASS_NAME, \"NewsEndMain_comp_article_main_news__0RmSO\"\n",
    "#                 )\n",
    "#                 .find_element(By.CLASS_NAME, \"NewsEndMain_image_media__rTvT1\")\n",
    "#                 .get_attribute(\"alt\")\n",
    "#             )\n",
    "#             date_results = driver.find_elements(\n",
    "#                 By.CLASS_NAME, \"NewsEndMain_date__xjtsQ\"\n",
    "#             )\n",
    "#             if len(date_results) > 1:\n",
    "#                 date = date_results[1].text\n",
    "#             else:\n",
    "#                 date = date_results[0].text\n",
    "#             date = convert_date(date)\n",
    "#         else:\n",
    "#             press = driver.find_element(\n",
    "#                 By.CLASS_NAME, \"NewsEndMain_highlight__HWvAi\"\n",
    "#             ).text\n",
    "#             date_results = driver.find_elements(By.CLASS_NAME, \"date\")\n",
    "#             if len(date_results) > 1:\n",
    "#                 date = date_results[1].text\n",
    "#             else:\n",
    "#                 date = date_results[0].text\n",
    "#             date = convert_date(date)\n",
    "\n",
    "#     return {\n",
    "#         \"title\": title,\n",
    "#         \"content\": content,\n",
    "#         \"press\": press,\n",
    "#         \"date\": date,\n",
    "#         \"reporter\": reporter,\n",
    "#         \"email\": email,\n",
    "# #     }\n",
    "\n",
    "\n",
    "# def crawl(\n",
    "#     url: str, category: str = None, chromedriver_path: str = \"/usr/bin/chromedriver\"\n",
    "# ):\n",
    "#     set_chromedriver(chromedriver_path)\n",
    "#     info = get_news_info(url, category)\n",
    "#     return info\n",
    "\n",
    "\n",
    "def get_news_info(\n",
    "    url: str,\n",
    "    category: str,\n",
    "):\n",
    "    driver.get(url)\n",
    "\n",
    "    driver.implicitly_wait(\n",
    "        2\n",
    "    )  # 2초 안에 웹페이지를 load하면 바로 넘어가거나, 2초를 기다림\n",
    "    # 사전 준비\n",
    "    email_pattern = (\n",
    "        r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}\"  # 이메일 정규표현식\n",
    "    )\n",
    "    reporter_pattern = r\"[ㄱ-힣]{2,4}\"  # 기자 이름 정규표현식\n",
    "\n",
    "    if category in [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"세계\", \"IT/과학\"]:\n",
    "        class_dict = {\n",
    "            \"title\": \"media_end_head_head_title\",\n",
    "            \"content\": \"_article_content\",\n",
    "            \"email_reporter\": \"byline\",\n",
    "        }\n",
    "        # title = _get_class_info(driver, class_dict[\"title\"])\n",
    "        # content = _get_class_info(driver, class_dict[\"content\"])\n",
    "        title = driver.find_element(By.CLASS_NAME, \"media_end_head_title\").text\n",
    "        content = driver.find_element(By.CLASS_NAME, \"_article_content\").text\n",
    "        email_text = driver.find_element(\n",
    "            By.CLASS_NAME, class_dict[\"email_reporter\"]\n",
    "        ).text\n",
    "        try:\n",
    "            email = re.findall(email_pattern, email_text)[0]\n",
    "        except:\n",
    "            email = \"None\"\n",
    "        try:\n",
    "            reporter = re.findall(reporter_pattern, email_text)[0]\n",
    "        except:\n",
    "            reporter = \"None\"\n",
    "\n",
    "        press = driver.find_element(\n",
    "            By.CLASS_NAME,\n",
    "            \"media_end_head_top_logo_text\",  # \"media_journalistcard_summary_press_img\"\n",
    "        ).get_attribute(\"alt\")\n",
    "\n",
    "        date_results = driver.find_elements(\n",
    "            By.CLASS_NAME, \"media_end_head_info_datestamp_bunch\"\n",
    "        )\n",
    "\n",
    "        if len(date_results) > 1:\n",
    "            date = driver.find_element(\n",
    "                By.CLASS_NAME, \"_ARTICLE_MODIFY_DATE_TIME\"\n",
    "            ).get_attribute(\"data-modify-date-time\")\n",
    "        else:\n",
    "            date = driver.find_element(\n",
    "                By.CLASS_NAME, \"_ARTICLE_DATE_TIME\"\n",
    "            ).get_attribute(\"data-date-time\")\n",
    "\n",
    "    elif category in [\"스포츠\", \"연예\"]:\n",
    "        class_dict = dict(\n",
    "            title=\"NewsEndMain_article_head_title__ztaL4\",\n",
    "            content=\"_article_content\",\n",
    "            email_reporter=\"NewsEndMain_article_journalist_info__Cdr3D\",\n",
    "        )\n",
    "        title, content = _get_class_info(driver, class_dict[\"title\"]), _get_class_info(\n",
    "            driver, class_dict[\"content\"]\n",
    "        )\n",
    "\n",
    "        email_text = driver.find_element(\n",
    "            By.CLASS_NAME, class_dict[\"email_reporter\"]\n",
    "        ).text\n",
    "        try:\n",
    "            email = re.findall(email_pattern, email_text)[0]\n",
    "        except:\n",
    "            email = \"None\"\n",
    "        try:\n",
    "            reporter = re.findall(reporter_pattern, email_text)[0]\n",
    "        except:\n",
    "            reporter = \"None\"\n",
    "\n",
    "        if category == \"스포츠\":\n",
    "            press = (\n",
    "                driver.find_element(\n",
    "                    By.CLASS_NAME, \"NewsEndMain_comp_article_main_news__0RmSO\"\n",
    "                )\n",
    "                .find_element(By.CLASS_NAME, \"NewsEndMain_image_media__rTvT1\")\n",
    "                .get_attribute(\"alt\")\n",
    "            )\n",
    "            date_results = driver.find_elements(\n",
    "                By.CLASS_NAME, \"NewsEndMain_date__xjtsQ\"\n",
    "            )\n",
    "            if len(date_results) > 1:\n",
    "                date = date_results[1].text\n",
    "            else:\n",
    "                date = date_results[0].text\n",
    "            date = convert_date(date)\n",
    "        else:\n",
    "            press = driver.find_element(\n",
    "                By.CLASS_NAME, \"NewsEndMain_highlight__HWvAi\"\n",
    "            ).text\n",
    "            date_results = driver.find_elements(By.CLASS_NAME, \"date\")\n",
    "            if len(date_results) > 1:\n",
    "                date = date_results[1].text\n",
    "            else:\n",
    "                date = date_results[0].text\n",
    "            date = convert_date(date)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"press\": press,\n",
    "        \"date\": date,\n",
    "        \"reporter\": reporter,\n",
    "        \"email\": email,\n",
    "    }\n",
    "\n",
    "\n",
    "# def crawl(\n",
    "#     url: str, category: str = None, chromedriver_path: str = \"/usr/bin/chromedriver\"\n",
    "# ):\n",
    "#     set_chromedriver(chromedriver_path)\n",
    "#     info = get_news_info(url, category)\n",
    "#     return info\n",
    "\n",
    "\n",
    "def crawl(\n",
    "    url: str, category: str = None, chromedriver_path: str = \"/usr/bin/chromedriver\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Crawl news with validation.\n",
    "    \"\"\"\n",
    "    # if not validate_news_url(url):\n",
    "    #     raise ValueError(\"Invalid news URL format\")\n",
    "\n",
    "    # url_info = extract_news_info(url)\n",
    "    # if not url_info:\n",
    "    #     raise ValueError(\"Could not extract news article identifiers\")\n",
    "\n",
    "    # Set up chrome driver and get info\n",
    "    set_chromedriver(chromedriver_path)\n",
    "    info = get_news_info(url, category)\n",
    "\n",
    "    # if not validate_news_metadata(info):\n",
    "    #     raise ValueError(\"Failed to crawl required news information\")\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException, NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "class URLExtractor:\n",
    "    def __init__(self, browser_type: str = 'chrome'):\n",
    "        \"\"\"\n",
    "        Initialize the URL extractor with specified browser.\n",
    "        \n",
    "        Args:\n",
    "            browser_type (str): Type of browser to use ('chrome' or 'firefox')\n",
    "        \"\"\"\n",
    "        self.setup_logging()\n",
    "        self.driver = self.setup_driver(browser_type)\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging settings\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        \n",
    "    def setup_driver(self, browser_type: str = \"chrome\", path : str = \"/usr/bin/chromedriver\"):\n",
    "        \"\"\"Set up and configure the WebDriver\"\"\"\n",
    "        try:\n",
    "            if browser_type.lower() == 'chrome':\n",
    "                # 크롬 드라이버 사용\n",
    "                service = Service(executable_path=path)  # selenium 최근 버전은 이렇게 해야함.\n",
    "\n",
    "                # 이런 것 때문에 기술문서를 보면서 코딩해야하고 영어도 잘해야함. (by. 스터디 팀장님)\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"--headless\")\n",
    "                options.add_argument(\"--no-sandbox\")\n",
    "                options.add_argument(\"--disable-dev-shm-usage\")\n",
    "                return webdriver.Chrome(options=options, service=service)\n",
    "            \n",
    "            elif browser_type.lower() == 'firefox':\n",
    "                options = webdriver.FirefoxOptions()\n",
    "                options.add_argument('--headless')\n",
    "                return webdriver.Firefox(options=options)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported browser type: {browser_type}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize WebDriver: {str(e)}\")\n",
    "            raise\n",
    "    def click_load_more(self) -> bool:\n",
    "        try:\n",
    "            button = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((\n",
    "                    By.CSS_SELECTOR, \n",
    "                    'a.section_more_inner'\n",
    "                ))\n",
    "            )\n",
    "            \n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", button)\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                button.click()\n",
    "            except:\n",
    "                self.driver.execute_script(\"arguments[0].click();\", button)\n",
    "\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.info(\"No more content to load\")\n",
    "            return False\n",
    "\n",
    "    def extract_urls(self, url: str, wait_time: int = 10) -> List[str]:\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            logging.info(\"Navigated to the target URL\")\n",
    "            \n",
    "            # Wait for initial content\n",
    "            WebDriverWait(self.driver, wait_time).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"sa_item\"))\n",
    "            )\n",
    "            \n",
    "            # Load all content\n",
    "            while True:\n",
    "                initial_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                \n",
    "                if not self.click_load_more():\n",
    "                    break\n",
    "                    \n",
    "                # Check if page height increased (new content loaded)\n",
    "                new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height <= initial_height:\n",
    "                    break\n",
    "            \n",
    "            # Extract URLs using specific selectors\n",
    "            urls = []\n",
    "            \n",
    "            # Find all news article containers\n",
    "            article_elements = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                'li.sa_item'\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Found {len(article_elements)} articles\")\n",
    "            \n",
    "            for article in article_elements:\n",
    "                # Get URL from either thumbnail or title link\n",
    "                links = article.find_elements(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    'a.sa_thumb_link, a.sa_text_title'\n",
    "                )\n",
    "                \n",
    "                for link in links:\n",
    "                    href = link.get_attribute('href')\n",
    "                    if href and 'news.naver.com/mnews/article' in href:\n",
    "                        urls.append(href)\n",
    "                        break  # Only take one URL per article\n",
    "            \n",
    "            unique_urls = list(dict.fromkeys(urls))\n",
    "            logging.info(f\"Successfully extracted {len(unique_urls)} unique article URLs\")\n",
    "            \n",
    "            return unique_urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting URLs: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logging.info(\"WebDriver closed successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 목록으로 부터 뉴스 url을 가져오는 함수\n",
    "set_chromedriver()\n",
    "category_dict = {\n",
    "    '정치':100,\n",
    "    '경제':101,\n",
    "    '사회':102,\n",
    "    '생활/문화':103,\n",
    "    'IT/과학': 105,\n",
    "    '세계':104,\n",
    "}\n",
    "subccateory_dict = {'대통령실':264, \"국회/정당\":265, \"북한\":268}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 11:02:32,003 - INFO - Navigated to the target URL\n",
      "2024-12-08 11:02:42,470 - INFO - No more content to load\n",
      "2024-12-08 11:02:42,517 - INFO - Found 46 articles\n",
      "2024-12-08 11:02:42,971 - INFO - Successfully extracted 45 unique article URLs\n"
     ]
    }
   ],
   "source": [
    "extractor = URLExtractor()\n",
    "category_idx = category_dict['정치']\n",
    "target_url = f\"https://news.naver.com/section/{category_idx}/\"\n",
    "urls = extractor.extract_urls(target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '“제발 저는 빼주세요”…尹 탄핵 표결 불참한 국힘 의원들에 ‘문자 폭탄’',\n",
       " 'content': '윤석열 대통령의 탄핵소추안 표결에 국민의힘 의원들이 불참하면서 탄핵안이 폐기되자 분노한 시민들이 여당 의원들에게 ‘문자 폭탄’을 보내고 있는 것으로 알려졌다. 엑스(옛 트위터) 캡처\\n\\n\\n윤석열 대통령의 탄핵소추안 표결에 국민의힘 의원들이 불참하면서 탄핵안이 폐기되자 분노한 시민들이 국민의힘 의원들에게 ‘문자 폭탄’을 보내고 있는 것으로 알려졌다.\\n\\n8일 정치권에 따르면 국민의힘 원내 관계자는 “비상계엄 선포 이후 문자·전화가 쏟아진다. 휴대전화를 ‘완충’해서 출근해도 보조배터리 없이는 2시간을 못 버틴다”며 “특정 단어를 차단 문구로 설정해도 특수문자를 넣어서 다시 보내는 통에 차단도 제대로 안 된다”고 연합뉴스에 전했다.\\n\\n온라인상에 올라온 인증 후기에 따르면 누리꾼들은 “민주주의 타령하면서 투표도 하러 오지 않는 건 뭐 하는 짓인지. 지금 당신들의 나태함 때문에 주말에 국민이 추운 날 길거리에 나와야겠냐”, “국민을 대표하는 자리를 포기한 의원은 국회의원 이전에 국민으로서의 자격이 없다” 등 비난이 담긴 내용으로 문자 메시지를 보냈다.\\n\\n이와 관련해 지난 7일 누리꾼 A씨는 “시위 참여가 어려우신 분들께. 지인이 국민의힘 국회의원들께 문자를 보내는 웹사이트를 개발했습니다”라며 한 웹사이트 링크를 공유하기도 했다.\\n\\n누리꾼 A씨가 공유한 웹사이트 링크. 엑스(옛 트위터) 캡처\\n\\n\\n해당 웹사이트를 만든 것으로 알려진 누리꾼 B씨는 지난 5일 “국민의힘에 단체문자 보내는 웹사이트 만들었습니다. 전화번호는 민주노총에서 올린 데이터를 참고했습니다”라고 설명했다.\\n\\n앞서 윤 대통령 탄핵안 표결을 전후해 이뤄진 국민의힘 의원총회 회의장 앞에서는 의원실 보좌진들이 충전된 보조배터리를 의원에게 전달하는 모습이 자주 목격됐다.\\n\\n업무에 필요한 연락조차 할 수 없는 상황이 이어지자 대구·경북(TK) 출신의 한 3선 의원은 전날 국민의힘 의원들이 모여있는 텔레그램 대화방에 연락처가 저장되지 않은 사람의 전화·문자를 차단하는 애플리케이션 다운로드 링크를 공유하기도 했다.\\n\\n이러한 상황에 현역 의원이 아닌 주요 당직자들도 ‘문자폭탄’ 피해를 호소했다.\\n\\n김재원 최고위원은 이날 자신의 페이스북에 “저 김재원은 국민의힘 최고위원이지만 국회의원이 아니다”라며 “대통령 탄핵소추 안건의 투표권이 없는데도 불구하고 어제부터 현재까지 수천 건의 욕설과 폭언 전화, 문자 메시지가 오고 있다. 제발 저는 빼주세요”라고 호소했다.\\n\\n국민의힘은 “개인정보인 국회의원의 휴대전화 번호를 무단 사용해 조직적·집단적으로 문자를 발송하는 위법행위가 발생하고 있다”며 “개인정보 유출과 업무방해 등 불법적인 행태에 대해서는 강력한 법적조치를 진행하겠다”고 밝혔다.\\n\\n경찰은 이날 여의도 국회 앞에 있는 국민의힘 당사 건물에 오물 투척 방지망을 설치했다. 여당 의원들의 탄핵안 표결 불참에 반발하는 시위대의 돌발 행동에 대비하려는 조치다.',\n",
       " 'press': None,\n",
       " 'date': '2024-12-08 16:44:17',\n",
       " 'reporter': '하승연',\n",
       " 'email': 'None'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl(urls[0], \"정치\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]2024-12-08 09:22:56,828 - INFO - Navigated to the target URL\n",
      "2024-12-08 09:23:07,139 - INFO - No more content to load\n",
      "2024-12-08 09:23:07,152 - INFO - Found 46 articles\n",
      "2024-12-08 09:23:07,599 - INFO - Successfully extracted 46 unique article URLs\n",
      "2024-12-08 09:23:07,670 - INFO - WebDriver closed successfully\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it]\n",
      " 17%|█▋        | 1/6 [00:26<02:14, 26.84s/it]2024-12-08 09:23:23,238 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0719a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,239 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c071d00>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,240 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c071bb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,241 - ERROR - Error extracting URLs: HTTPConnectionPool(host='localhost', port=44275): Max retries exceeded with url: /session/23e63060a593038b18bfe08e16541a17/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083370>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "2024-12-08 09:23:23,242 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083280>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,243 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0833a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,244 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0832e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,245 - INFO - WebDriver closed successfully\n",
      "0it [00:00, ?it/s]\n",
      "2024-12-08 09:23:23,249 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083e50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,250 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083be0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,251 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0270a0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,251 - ERROR - Error extracting URLs: HTTPConnectionPool(host='localhost', port=44275): Max retries exceeded with url: /session/23e63060a593038b18bfe08e16541a17/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c027220>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "2024-12-08 09:23:23,252 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c027460>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,253 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0275b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,254 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0277f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,255 - INFO - WebDriver closed successfully\n",
      "0it [00:00, ?it/s]\n",
      "2024-12-08 09:23:23,259 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083f70>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,260 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083a90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,260 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0830d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,261 - ERROR - Error extracting URLs: HTTPConnectionPool(host='localhost', port=44275): Max retries exceeded with url: /session/23e63060a593038b18bfe08e16541a17/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0832b0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "2024-12-08 09:23:23,262 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083460>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,263 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083100>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,264 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c027ac0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,265 - INFO - WebDriver closed successfully\n",
      "0it [00:00, ?it/s]\n",
      "2024-12-08 09:23:23,268 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c027280>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,269 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c027550>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,270 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c027a30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,271 - ERROR - Error extracting URLs: HTTPConnectionPool(host='localhost', port=44275): Max retries exceeded with url: /session/23e63060a593038b18bfe08e16541a17/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c030190>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "2024-12-08 09:23:23,272 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c030340>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,273 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0304c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,275 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1ca7ed30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,276 - INFO - WebDriver closed successfully\n",
      "0it [00:00, ?it/s]\n",
      "2024-12-08 09:23:23,279 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083970>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,280 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c0836d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,281 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083940>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17/url\n",
      "2024-12-08 09:23:23,282 - ERROR - Error extracting URLs: HTTPConnectionPool(host='localhost', port=44275): Max retries exceeded with url: /session/23e63060a593038b18bfe08e16541a17/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083f70>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "2024-12-08 09:23:23,283 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083ac0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,284 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c083820>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,285 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2d1c071b50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/23e63060a593038b18bfe08e16541a17\n",
      "2024-12-08 09:23:23,286 - INFO - WebDriver closed successfully\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 6/6 [00:26<00:00,  4.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Initialize the extractor\n",
    "extractor = URLExtractor()\n",
    "for category in tqdm(['정치','경제','사회','생활/문화','IT/과학','세계']):\n",
    "    category_idx = category_dict[category]\n",
    "    try:\n",
    "        # Replace with your target URL\n",
    "        target_url = f\"https://news.naver.com/section/{category_idx}/\"\n",
    "        urls = extractor.extract_urls(target_url)\n",
    "            \n",
    "    finally:\n",
    "        # Always close the browser\n",
    "        extractor.close()\n",
    "\n",
    "\n",
    "    for u in tqdm(urls[:10]):\n",
    "        crawl(u, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from typing import List\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def set_chromedriver(path: str = \"/usr/bin/chromedriver\"):\n",
    "    global driver\n",
    "    # 크롬 드라이버 사용\n",
    "    service = Service(executable_path=path)  # selenium 최근 버전은 이렇게 해야함.\n",
    "\n",
    "    # 이런 것 때문에 기술문서를 보면서 코딩해야하고 영어도 잘해야함. (by. 스터디 팀장님)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=options, service=service)\n",
    "\n",
    "\n",
    "def convert_date(date):\n",
    "    ymd, dn, hm = date.split(\" \")\n",
    "    ymd = ymd[:-1].replace(\".\", \"-\")\n",
    "    hours = 12 if dn == \"오후\" else 0\n",
    "    h = int(hm.split(\":\")[0]) + hours\n",
    "    m = hm.split(\":\")[1]\n",
    "    return f\"{ymd} {h}:{m}\"\n",
    "\n",
    "\n",
    "def _get_class_info(\n",
    "    driver,\n",
    "    class_name,\n",
    "):\n",
    "    return driver.find_element(By.CLASS_NAME, class_name).text\n",
    "\n",
    "\n",
    "def get_news_info(url: str, category: str):\n",
    "    \"\"\"\n",
    "    Extract news information from URL with improved error handling.\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # Regular expressions\n",
    "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}\"\n",
    "    reporter_pattern = r\"[ㄱ-힣]{2,4}\"\n",
    "\n",
    "    # Initialize default values\n",
    "    title = \"\"\n",
    "    content = \"\"\n",
    "    press = \"Unknown\"\n",
    "    date = \"\"\n",
    "    reporter = \"None\"\n",
    "    email = \"None\"\n",
    "\n",
    "    try:\n",
    "        if category in [\"정치\", \"경제\", \"사회\", \"생활/문화\", \"IT/과학\", \"세계\"]:\n",
    "            # Regular news\n",
    "            try:\n",
    "                title = driver.find_element(By.CLASS_NAME, \"media_end_head_title\").text\n",
    "                content = driver.find_element(By.CLASS_NAME, \"_article_content\").text\n",
    "\n",
    "                # Reporter info\n",
    "                try:\n",
    "                    email_text = driver.find_element(By.CLASS_NAME, \"byline\").text\n",
    "                    email_matches = re.findall(email_pattern, email_text)\n",
    "                    reporter_matches = re.findall(reporter_pattern, email_text)\n",
    "\n",
    "                    email = email_matches[0] if email_matches else \"None\"\n",
    "                    reporter = reporter_matches[0] if reporter_matches else \"None\"\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error extracting reporter info: {str(e)}\")\n",
    "\n",
    "                # Press info\n",
    "                try:\n",
    "                    press_element = driver.find_element(\n",
    "                        By.CLASS_NAME, \"media_end_head_top_logo_text\"\n",
    "                    )\n",
    "                    press = press_element.get_attribute(\"alt\") or \"Unknown\"\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error extracting press info: {str(e)}\")\n",
    "\n",
    "                # Date info\n",
    "                try:\n",
    "                    date_results = driver.find_elements(\n",
    "                        By.CLASS_NAME, \"media_end_head_info_datestamp_bunch\"\n",
    "                    )\n",
    "                    if len(date_results) > 1:\n",
    "                        date = driver.find_element(\n",
    "                            By.CLASS_NAME, \"_ARTICLE_MODIFY_DATE_TIME\"\n",
    "                        ).get_attribute(\"data-modify-date-time\")\n",
    "                    else:\n",
    "                        date = driver.find_element(\n",
    "                            By.CLASS_NAME, \"_ARTICLE_DATE_TIME\"\n",
    "                        ).get_attribute(\"data-date-time\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error extracting date: {str(e)}\")\n",
    "                    date = \"Unknown\"\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error extracting regular news content: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        elif category in [\"스포츠\", \"연예\"]:\n",
    "            # Sports and Entertainment news\n",
    "            class_dict = {\n",
    "                \"title\": \"NewsEndMain_article_head_title__ztaL4\",\n",
    "                \"content\": \"_article_content\",\n",
    "                \"email_reporter\": \"NewsEndMain_article_journalist_info__Cdr3D\",\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                title = _get_class_info(driver, class_dict[\"title\"])\n",
    "                content = _get_class_info(driver, class_dict[\"content\"])\n",
    "\n",
    "                # Reporter info\n",
    "                try:\n",
    "                    email_text = driver.find_element(\n",
    "                        By.CLASS_NAME, class_dict[\"email_reporter\"]\n",
    "                    ).text\n",
    "                    email_matches = re.findall(email_pattern, email_text)\n",
    "                    reporter_matches = re.findall(reporter_pattern, email_text)\n",
    "\n",
    "                    email = email_matches[0] if email_matches else \"None\"\n",
    "                    reporter = reporter_matches[0] if reporter_matches else \"None\"\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error extracting reporter info: {str(e)}\")\n",
    "\n",
    "                # Category specific handling\n",
    "                if category == \"스포츠\":\n",
    "                    try:\n",
    "                        press = (\n",
    "                            driver.find_element(\n",
    "                                By.CLASS_NAME,\n",
    "                                \"NewsEndMain_comp_article_main_news__0RmSO\",\n",
    "                            )\n",
    "                            .find_element(\n",
    "                                By.CLASS_NAME, \"NewsEndMain_image_media__rTvT1\"\n",
    "                            )\n",
    "                            .get_attribute(\"alt\")\n",
    "                        ) or \"Unknown\"\n",
    "\n",
    "                        date_results = driver.find_elements(\n",
    "                            By.CLASS_NAME, \"NewsEndMain_date__xjtsQ\"\n",
    "                        )\n",
    "                        date = (\n",
    "                            date_results[1].text\n",
    "                            if len(date_results) > 1\n",
    "                            else date_results[0].text\n",
    "                        )\n",
    "                        date = convert_date(date)\n",
    "                    except Exception as e:\n",
    "                        logging.warning(\n",
    "                            f\"Error extracting sports specific info: {str(e)}\"\n",
    "                        )\n",
    "                else:  # Entertainment\n",
    "                    try:\n",
    "                        press = (\n",
    "                            driver.find_element(\n",
    "                                By.CLASS_NAME, \"NewsEndMain_highlight__HWvAi\"\n",
    "                            ).text\n",
    "                            or \"Unknown\"\n",
    "                        )\n",
    "\n",
    "                        date_results = driver.find_elements(By.CLASS_NAME, \"date\")\n",
    "                        date = (\n",
    "                            date_results[1].text\n",
    "                            if len(date_results) > 1\n",
    "                            else date_results[0].text\n",
    "                        )\n",
    "                        date = convert_date(date)\n",
    "                    except Exception as e:\n",
    "                        logging.warning(\n",
    "                            f\"Error extracting entertainment specific info: {str(e)}\"\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(\n",
    "                    f\"Error extracting sports/entertainment content: {str(e)}\"\n",
    "                )\n",
    "                raise\n",
    "\n",
    "        # Validate required fields\n",
    "        if not title or not content:\n",
    "            raise ValueError(\"Required fields (title/content) missing\")\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"press\": press or \"Unknown\",\n",
    "            \"date\": date or \"Unknown\",\n",
    "            \"reporter\": reporter,\n",
    "            \"email\": email,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing URL {url}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def crawl(\n",
    "    url: str, category: str = None, chromedriver_path: str = \"/usr/bin/chromedriver\"\n",
    "):\n",
    "    # Set up chrome driver and get info\n",
    "\n",
    "    set_chromedriver(chromedriver_path)\n",
    "    info = get_news_info(url, category)\n",
    "    # logger.info(f\"Crawl successful: {info}\")/\n",
    "\n",
    "    return info\n",
    "\n",
    "\n",
    "class URLExtractor:\n",
    "    def __init__(self, browser_type: str = \"chrome\"):\n",
    "        \"\"\"\n",
    "        Initialize the URL extractor with specified browser.\n",
    "\n",
    "        Args:\n",
    "            browser_type (str): Type of browser to use ('chrome' or 'firefox')\n",
    "        \"\"\"\n",
    "        self.setup_logging()\n",
    "        self.driver = self.setup_driver(browser_type)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging settings\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "        )\n",
    "\n",
    "    def setup_driver(\n",
    "        self, browser_type: str = \"chrome\", path: str = \"/usr/bin/chromedriver\"\n",
    "    ):\n",
    "        \"\"\"Set up and configure the WebDriver\"\"\"\n",
    "        try:\n",
    "            if browser_type.lower() == \"chrome\":\n",
    "                # 크롬 드라이버 사용\n",
    "                service = Service(\n",
    "                    executable_path=path\n",
    "                )  # selenium 최근 버전은 이렇게 해야함.\n",
    "\n",
    "                # 이런 것 때문에 기술문서를 보면서 코딩해야하고 영어도 잘해야함. (by. 스터디 팀장님)\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"--headless\")\n",
    "                options.add_argument(\"--no-sandbox\")\n",
    "                options.add_argument(\"--disable-dev-shm-usage\")\n",
    "                return webdriver.Chrome(options=options, service=service)\n",
    "\n",
    "            elif browser_type.lower() == \"firefox\":\n",
    "                options = webdriver.FirefoxOptions()\n",
    "                options.add_argument(\"--headless\")\n",
    "                return webdriver.Firefox(options=options)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported browser type: {browser_type}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize WebDriver: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_urls(self, url: str, wait_time: int = 5) -> List[str]:\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            logging.info(\"Navigated to the target URL\")\n",
    "\n",
    "            # Extract URLs using specific selectors\n",
    "            urls = []\n",
    "\n",
    "            # Find all news article containers\n",
    "            article_elements = self.driver.find_elements(By.CSS_SELECTOR, \"li.sa_item\")\n",
    "\n",
    "            logging.info(f\"Found {len(article_elements)} articles\")\n",
    "\n",
    "            for article in article_elements:\n",
    "                # Get URL from either thumbnail or title link\n",
    "                links = article.find_elements(\n",
    "                    By.CSS_SELECTOR, \"a.sa_thumb_link, a.sa_text_title\"\n",
    "                )\n",
    "\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if href and \"news.naver.com/mnews/article\" in href:\n",
    "                        urls.append(href)\n",
    "                        break  # Only take one URL per article\n",
    "\n",
    "            unique_urls = list(dict.fromkeys(urls))\n",
    "            logging.info(\n",
    "                f\"Successfully extracted {len(unique_urls)} unique article URLs\"\n",
    "            )\n",
    "\n",
    "            return unique_urls\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting URLs: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logging.info(\"WebDriver closed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.driver.find_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library imports\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "# Selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "\n",
    "class URLExtractor:\n",
    "    # URL patterns for different sections\n",
    "    URL_PATTERNS = {\n",
    "        'news': r'https://news\\.naver\\.com/section/\\d+',\n",
    "        'entertainment': r'https://entertain\\.naver\\.com/now',\n",
    "        'sports': r'https://sports\\.news\\.naver\\.com/\\w+/news/index\\?isphoto=N'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, browser_type: str = \"chrome\"):\n",
    "        self.setup_logging()\n",
    "        self.driver = self.setup_driver(browser_type)\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "        )\n",
    "\n",
    "    def setup_driver(self, browser_type: str = \"chrome\", path: str = \"/usr/bin/chromedriver\"):\n",
    "        try:\n",
    "            if browser_type.lower() == \"chrome\":\n",
    "                service = Service(executable_path=path)\n",
    "                options = webdriver.ChromeOptions()\n",
    "                options.add_argument(\"--headless\")\n",
    "                options.add_argument(\"--no-sandbox\")\n",
    "                options.add_argument(\"--disable-dev-shm-usage\")\n",
    "                return webdriver.Chrome(options=options, service=service)\n",
    "            elif browser_type.lower() == \"firefox\":\n",
    "                options = webdriver.FirefoxOptions()\n",
    "                options.add_argument(\"--headless\")\n",
    "                return webdriver.Firefox(options=options)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported browser type: {browser_type}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize WebDriver: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def detect_url_type(self, url: str) -> str:\n",
    "        \"\"\"Detect the type of URL based on patterns\"\"\"\n",
    "        for url_type, pattern in self.URL_PATTERNS.items():\n",
    "            if re.match(pattern, url):\n",
    "                return url_type\n",
    "        raise ValueError(f\"URL does not match any known pattern: {url}\")\n",
    "\n",
    "    def extract_urls_news(self, url: str, count_clicks: int = 3) -> List[str]:\n",
    "        \"\"\"Extract URLs from news section\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            meta_element = self.driver.find_element(\n",
    "                By.CSS_SELECTOR, \n",
    "                \"#newsct > div.section_latest > div > div.section_latest_article._CONTENT_LIST._PERSIST_META\"\n",
    "            )\n",
    "            \n",
    "            urls = []\n",
    "            click_count = 0\n",
    "            \n",
    "            # Initial articles\n",
    "            article_links = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \"a.sa_text_title._NLOG_IMPRESSION\"\n",
    "            )\n",
    "            for link in article_links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href and \"news.naver.com/mnews/article\" in href:\n",
    "                    urls.append(href)\n",
    "            \n",
    "            # Click \"더보기\" button specified number of times\n",
    "            while click_count < count_clicks:\n",
    "                try:\n",
    "                    button = WebDriverWait(self.driver, 10).until(\n",
    "                        EC.element_to_be_clickable(\n",
    "                            (By.CSS_SELECTOR, \"a.section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON\")\n",
    "                        )\n",
    "                    )\n",
    "                    self.driver.execute_script(\"arguments[0].click();\", button)\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    # Get new articles\n",
    "                    article_links = self.driver.find_elements(\n",
    "                        By.CSS_SELECTOR, \"a.sa_text_title._NLOG_IMPRESSION\"\n",
    "                    )\n",
    "                    for link in article_links:\n",
    "                        href = link.get_attribute(\"href\")\n",
    "                        if href and \"news.naver.com/mnews/article\" in href:\n",
    "                            urls.append(href)\n",
    "                            \n",
    "                    click_count += 1\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error clicking more button: {str(e)}\")\n",
    "                    break\n",
    "                \n",
    "            return list(dict.fromkeys(urls))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in news extraction: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def extract_urls_entertainment(self, url: str) -> List[str]:\n",
    "        \"\"\"Extract URLs from entertainment section\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            urls = []\n",
    "            \n",
    "            # Wait for content to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"#newsWrp ul\"))\n",
    "            )\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Get articles using the exact querySelector path\n",
    "            article_links = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                \"#newsWrp > ul > li > div > a\"\n",
    "            )\n",
    "            \n",
    "            for link in article_links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href:\n",
    "                    urls.append(href)\n",
    "            \n",
    "            logging.info(f\"Found {len(urls)} entertainment URLs\")\n",
    "            return list(dict.fromkeys(urls))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in entertainment extraction: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def extract_urls_sports(self, url: str) -> List[str]:\n",
    "        \"\"\"Extract URLs from sports section\"\"\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            urls = []\n",
    "            \n",
    "            # Wait for content to load\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.news_list\"))\n",
    "            )\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Get articles\n",
    "            article_links = self.driver.find_elements(\n",
    "                By.CSS_SELECTOR, \n",
    "                \"div.news_list a.title\"\n",
    "            )\n",
    "            \n",
    "            for link in article_links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href:\n",
    "                    urls.append(href)\n",
    "            \n",
    "            return list(dict.fromkeys(urls))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in sports extraction: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def extract_urls(self, url: str, count_clicks: int = 3) -> List[str]:\n",
    "        \"\"\"Main method to extract URLs based on URL pattern\"\"\"\n",
    "        url_type = self.detect_url_type(url)\n",
    "        logging.info(f\"Detected URL type: {url_type}\")\n",
    "        \n",
    "        if url_type == \"news\":\n",
    "            return self.extract_urls_news(url, count_clicks)\n",
    "        elif url_type == \"entertainment\":\n",
    "            return self.extract_urls_entertainment(url)\n",
    "        elif url_type == \"sports\":\n",
    "            return self.extract_urls_sports(url)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported URL type: {url_type}\")\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up WebDriver when done\"\"\"\n",
    "        if hasattr(self, 'driver'):\n",
    "            self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 11:40:39,412 - INFO - Detected URL type: news\n",
      "2024-12-15 11:40:47,745 - INFO - Detected URL type: entertainment\n",
      "2024-12-15 11:40:50,476 - INFO - Found 25 entertainment URLs\n",
      "2024-12-15 11:40:50,477 - INFO - Detected URL type: sports\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News URLs: 154\n",
      "Entertainment URLs: 25\n",
      "Sports URLs: 20\n"
     ]
    }
   ],
   "source": [
    "extractor = URLExtractor()\n",
    "\n",
    "# Example URLs for each type\n",
    "news_url = \"https://news.naver.com/section/101\"\n",
    "entertainment_url = \"https://entertain.naver.com/now#sid=106&date=2024-12-15&page=7\"\n",
    "sports_url = \"https://sports.news.naver.com/kbaseball/news/index?isphoto=N&page=2\"\n",
    "\n",
    "# Extract URLs based on type\n",
    "news_urls = extractor.extract_urls(news_url, count_clicks=3)\n",
    "entertainment_urls = extractor.extract_urls(entertainment_url)\n",
    "sports_urls = extractor.extract_urls(sports_url)\n",
    "\n",
    "# Print results\n",
    "print(f\"News URLs: {len(news_urls)}\")\n",
    "print(f\"Entertainment URLs: {len(entertainment_urls)}\")\n",
    "print(f\"Sports URLs: {len(sports_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
